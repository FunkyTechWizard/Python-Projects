{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##Utsav Patel\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6mCe9Fk5LOms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n"
      ],
      "metadata": {
        "id": "u9_i_IBT_4tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing google drive path\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFc5Zr3p9XmM",
        "outputId": "efb2b34d-4188-4001-ddd2-34c18c925425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing csv file\n",
        "import os"
      ],
      "metadata": {
        "id": "nbC4RnOd9iRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/Assign09/HAR_Images')"
      ],
      "metadata": {
        "id": "yRT3Xyvc9iXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets='/content/drive/MyDrive/Assign09/HAR_Images'"
      ],
      "metadata": {
        "id": "6uMOyXqx-PBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transforms for preprocessing the data\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "5WCrfIPmLffP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['Catch', 'Clap', 'Hammering']"
      ],
      "metadata": {
        "id": "TVO4fquF_C40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "file_names = os.listdir(datasets)\n",
        "\n",
        "class_labels = [file_name.split('_')[0] for file_name in file_names]\n",
        "\n",
        "train_files, test_files, train_labels, test_labels = train_test_split(file_names, class_labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "2LQVkeJG_VGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the datasets\n",
        "train_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/Assign09/HAR_Images', \n",
        "                                      transform=train_transforms)\n",
        "test_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/Assign09/HAR_Images', \n",
        "                                     transform=test_transforms)\n",
        "\n",
        "# Define the dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "-dovmhUiLdRT",
        "outputId": "44f4a645-9d19-4c68-8a51-9de7fbe96c8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-ed82fa0c9892>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m train_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/Assign09/HAR_Images', \n\u001b[0m\u001b[1;32m      3\u001b[0m                                       transform=train_transforms)\n\u001b[1;32m      4\u001b[0m test_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/Assign09/HAR_Images', \n\u001b[1;32m      5\u001b[0m                                      transform=test_transforms)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'ImageFolder'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data into PyTorch datasets\n",
        "train_data = datasets.ImageFolder(data_dir + '/train', transform=transform)\n",
        "test_data = datasets.ImageFolder(data_dir + '/test', transform=transform)"
      ],
      "metadata": {
        "id": "c-1u4tv2fRu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "train_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/Assign09/HAR_Images', transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/Assign09/HAR_Images', transform=transform)\n"
      ],
      "metadata": {
        "id": "3SH5wJgmwF1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])"
      ],
      "metadata": {
        "id": "flRdydS1wV7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the data loaders for training and testing sets\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "QU-b7zl3wcXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the class labels\n",
        "classes = dataset.classes\n",
        "num_classes = len(classes)"
      ],
      "metadata": {
        "id": "DhnnNucKQeSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 6 * 6, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 6 * 6)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "oSP-YcEYMmRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model and move it to GPU if available\n",
        "net = Net()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "net.to(device)"
      ],
      "metadata": {
        "id": "Lad4Ts1AM2qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "jn6PP8QtNCfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    \n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = 100 * correct / total\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    print('Epoch: %d, Training Loss: %.3f, Training Accuracy: %.3f' % (epoch+1, train_loss, train_accuracy))\n",
        "    \n"
      ],
      "metadata": {
        "id": "yshYjPB5NzNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score, ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "\n",
        "sns.heatmap(confusion_matrix(y_test,y_pred), annot=True, cmap='YlGnBu')\n",
        "\n",
        "# Add labels and title to the plot\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "PpvrU8CJOj8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Metrics = pd.DataFrame({\"Accuracy\":[accuracy_score(y_test, y_pred, normalize=True)]\n",
        ",\"Precision\":[precision_score(y_test, y_pred, average = 'macro')]\n",
        ",\"Recall\":[recall_score(y_test, y_pred, average = 'macro')]\n",
        ",\"F1-Score\":[f1_score(y_test, y_pred, average = 'macro')]})"
      ],
      "metadata": {
        "id": "cfOTbqrVOoVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Metrics"
      ],
      "metadata": {
        "id": "-RYiTVrfOr65"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}